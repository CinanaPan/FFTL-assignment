import os
import re
from datetime import datetime
import spacy
import matplotlib.pyplot as plt
from collections import defaultdict

# 加载 spaCy 的中文模型
nlp = spacy.load("zh_core_web_sm")

# 从文件名中提取日期和歌曲名
def extract_metadata(file_name):
    file_name = file_name.strip()
    print(f"Extracting metadata from cleaned file name: {repr(file_name)}")

    # 匹配完整日期格式 YYYY-MM-DD
    match_full = re.match(r"^(\d{4}-\d{2}-\d{2})_(.+)\.txt$", file_name)
    if match_full:
        date, song_name = match_full.groups()
        return date, song_name

    # 匹配年份和月份格式 YYYY-MM
    match_month = re.match(r"^(\d{4}-\d{2})_(.+)\.txt$", file_name)
    if match_month:
        date, song_name = match_month.groups()
        date = f"{date}-01"  # 补全为第一天
        return date, song_name

    # 匹配仅年份格式 YYYY
    match_year = re.match(r"^(\d{4})_(.+)\.txt$", file_name)
    if match_year:
        date, song_name = match_year.groups()
        date = f"{date}-01-01"  # 补全为年初
        return date, song_name

    # 无法匹配的文件名
    return None, None

# 计算句法树深度
def calculate_tree_depth(token):
    if not list(token.children):
        return 1
    return 1 + max(calculate_tree_depth(child) for child in token.children)

# 分析单个文件的句法树深度
def analyze_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()

    # spaCy 分析文本内容
    doc = nlp(content)
    depths = []

    # 遍历每个句子，计算句法树深度
    for sent in doc.sents:
        root = [token for token in sent if token.head == token][0]  # 找到句子的根节点
        depth = calculate_tree_depth(root)
        depths.append(depth)

    # 返回句子的平均句法树深度
    return sum(depths) / len(depths) if depths else 0

# 批量分析文件并按年份计算平均句法深度
def analyze_syntax_by_year(corpus_path):
    year_depths = defaultdict(list)

    for file_name in os.listdir(corpus_path):
        if file_name.endswith(".txt"):
            file_path = os.path.join(corpus_path, file_name)
            date, _ = extract_metadata(file_name)

            if date:
                year = datetime.strptime(date, "%Y-%m-%d").year
                avg_depth = analyze_file(file_path)
                year_depths[year].append(avg_depth)

    # 计算每年的平均句法深度
    year_avg_depth = {year: sum(depths) / len(depths) for year, depths in year_depths.items()}
    return year_avg_depth

# 可视化平均句法深度
def plot_syntax_depth(year_avg_depth, output_path=None):
    years = sorted(year_avg_depth.keys())
    avg_depths = [year_avg_depth[year] for year in years]

    plt.figure(figsize=(10, 6))
    plt.plot(years, avg_depths, marker='o', label="Average Syntax Tree Depth")
    plt.title("Average Syntax Tree Depth Over Years", fontsize=16)
    plt.xlabel("Year", fontsize=14)
    plt.ylabel("Syntax Tree Depth", fontsize=14)
    plt.grid(True)
    plt.legend(fontsize=12)

    # 保存或展示图表
    if output_path:
        plt.savefig(output_path, format='png', dpi=300)
        print(f"Syntax depth plot saved to {output_path}")
    plt.show()

# 主程序
def main():
    # 设置语料库路径
    corpus_path = r"C:\Users\PC\Desktop\PAN-24332220-FFLT-Corpus\1995-2014Lyrics_ChinesePopSong_Parsed_annotated_crawedfromNetease"

    # 分析句法复杂性并按年份汇总
    year_avg_depth = analyze_syntax_by_year(corpus_path)

    # 输出结果
    print("Yearly Average Syntax Tree Depth:")
    for year, avg_depth in sorted(year_avg_depth.items()):
        print(f"Year: {year}, Average Depth: {avg_depth:.2f}")

    # 可视化句法树深度趋势
    output_path = os.path.join(corpus_path, 'syntax_tree_depth_trends.png')
    plot_syntax_depth(year_avg_depth, output_path)

if __name__ == "__main__":
    main()
